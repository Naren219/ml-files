### activation functions
* relu doesn't suffer from over-saturations when values are near the min/max
