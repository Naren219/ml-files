{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "**A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, ConvNets have the ability to learn these filters/characteristics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "36b69b0a-4c98-4c75-b489-d062a6b650e2",
    "_kg_hide-input": true,
    "_uuid": "301458b7-59c9-4442-9e19-c1db75fedce0",
    "execution": {
     "iopub.execute_input": "2022-07-09T16:08:27.556375Z",
     "iopub.status.busy": "2022-07-09T16:08:27.555864Z",
     "iopub.status.idle": "2022-07-09T16:08:27.562352Z",
     "shell.execute_reply": "2022-07-09T16:08:27.561058Z",
     "shell.execute_reply.started": "2022-07-09T16:08:27.556332Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from string import Template\n",
    "import IPython.display\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T16:08:27.565458Z",
     "iopub.status.busy": "2022-07-09T16:08:27.564742Z",
     "iopub.status.idle": "2022-07-09T16:08:27.579259Z",
     "shell.execute_reply": "2022-07-09T16:08:27.578515Z",
     "shell.execute_reply.started": "2022-07-09T16:08:27.565413Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv:\n",
    "    \n",
    "    def __init__(self, num_filters):\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        #why divide by 9...Xavier initialization\n",
    "        self.filters = np.random.randn(num_filters, 3, 3)/9\n",
    "    \n",
    "    def iterate_regions(self, image):\n",
    "        h,w = image.shape\n",
    "\n",
    "        #generates all possible 3*3 image regions using valid padding\n",
    "        \n",
    "        for i in range(h-2):\n",
    "            for j in range(w-2):\n",
    "                im_region = image[i:(i+3), j:(j+3)]\n",
    "                yield im_region, i, j\n",
    "                \n",
    "    def forward(self, input):\n",
    "        self.last_input = input\n",
    "        \n",
    "        h,w = input.shape\n",
    "        output = np.zeros((h-2, w-2, self.num_filters))\n",
    "        \n",
    "        for im_regions, i, j in self.iterate_regions(input):\n",
    "            output[i, j] = np.sum(im_regions * self.filters, axis=(1,2))\n",
    "        return output\n",
    "    \n",
    "    def backprop(self, d_l_d_out, learn_rate):\n",
    "        '''\n",
    "        Performs a backward pass of the conv layer.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        - learn_rate is a float.\n",
    "        '''\n",
    "        d_l_d_filters = np.zeros(self.filters.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "            for f in range(self.num_filters):\n",
    "                d_l_d_filters[f] += d_l_d_out[i,j,f] * im_region\n",
    "\n",
    "        #update filters\n",
    "        self.filters -= learn_rate * d_l_d_filters\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxPooling Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A Max Pooling layer can’t be trained because it doesn’t actually have any weights, but we still need to implement a backprop() method for it to calculate gradients. We’ll start by adding forward phase caching again. All we need to cache this time is the input:*\n",
    "\n",
    "*During the forward pass, the Max Pooling layer takes an input volume and halves its width and height dimensions by picking the max values over 2x2 blocks. The backward pass does the opposite: we’ll double the width and height of the loss gradient by assigning each gradient value to where the original max value was in its corresponding 2x2 block.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T16:08:27.581523Z",
     "iopub.status.busy": "2022-07-09T16:08:27.580846Z",
     "iopub.status.idle": "2022-07-09T16:08:27.595329Z",
     "shell.execute_reply": "2022-07-09T16:08:27.594349Z",
     "shell.execute_reply.started": "2022-07-09T16:08:27.581480Z"
    }
   },
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "    def iterate_regions(self, image):\n",
    "        h, w, _ = image.shape\n",
    "        \n",
    "        # floor division\n",
    "        new_h = h // 2\n",
    "        new_w = w // 2\n",
    "        \n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i*2):(i*2+2), (j*2):(j*2+2)]\n",
    "                yield im_region, i, j\n",
    "                \n",
    "    def forward(self, input):\n",
    "        \n",
    "        self.last_input = input\n",
    "        \n",
    "        h, w, num_filters = input.shape\n",
    "        output = np.zeros((h//2, w//2, num_filters))\n",
    "        \n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i,j] = np.amax(im_region,axis=(0,1))\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def backprop(self, d_l_d_out):\n",
    "        '''\n",
    "        Performs a backward pass of the maxpool layer.\n",
    "        Returns the loss gradient for this layer's inputs.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        '''\n",
    "        d_l_d_input = np.zeros(self.last_input.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "            h, w, f = im_region.shape\n",
    "            amax = np.amax(im_region, axis=(0,1))\n",
    "\n",
    "            for i2 in range(h):\n",
    "                for j2 in range(w):\n",
    "                    for f2 in range(f):\n",
    "                        #if the pixel was the max value, copy the gradient to it\n",
    "                        if(im_region[i2,j2,f2] == amax[f2]):\n",
    "                            d_l_d_input[i*2+i2, j*2+j2 ,f2] = d_l_d_out[i, j, f2]\n",
    "                            break;\n",
    "        return d_l_d_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T16:08:27.597739Z",
     "iopub.status.busy": "2022-07-09T16:08:27.596774Z",
     "iopub.status.idle": "2022-07-09T16:08:27.613890Z",
     "shell.execute_reply": "2022-07-09T16:08:27.612814Z",
     "shell.execute_reply.started": "2022-07-09T16:08:27.597692Z"
    }
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self, input_len, nodes):\n",
    "        # We divide by input_len to reduce the variance of our initial values\n",
    "        self.weights = np.random.randn(input_len, nodes)/input_len\n",
    "        self.biases = np.zeros(nodes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        self.last_input_shape = input.shape\n",
    "        \n",
    "        input = input.flatten()\n",
    "        self.last_input = input\n",
    "        \n",
    "        input_len, nodes = self.weights.shape\n",
    "        \n",
    "        totals = np.dot(input, self.weights) + self.biases\n",
    "        self.last_totals = totals\n",
    "        \n",
    "        exp = np.exp(totals)\n",
    "        return(exp/np.sum(exp, axis=0)) \n",
    "    \n",
    "    def backprop(self, d_l_d_out, learn_rate):\n",
    "        \"\"\"  \n",
    "        Performs a backward pass of the softmax layer.\n",
    "        Returns the loss gradient for this layers inputs.\n",
    "        - d_L_d_out is the loss gradient for this layers outputs.\n",
    "        \"\"\"\n",
    "        \n",
    "        #We know only 1 element of d_l_d_out will be nonzero\n",
    "        for i, gradient in enumerate(d_l_d_out):\n",
    "            if(gradient == 0):\n",
    "                continue\n",
    "            \n",
    "            #e^totals\n",
    "            t_exp = np.exp(self.last_totals)\n",
    "            \n",
    "            #Sum of all e^totals\n",
    "            S = np.sum(t_exp)\n",
    "            \n",
    "            #gradients of out[i] against totals\n",
    "            d_out_d_t = -t_exp[i] * t_exp/ (S**2)\n",
    "            d_out_d_t[i] = t_exp[i] * (S-t_exp[i]) /(S**2)\n",
    "            \n",
    "            # Gradients of totals against weights/biases/input\n",
    "            d_t_d_w = self.last_input\n",
    "            d_t_d_b = 1\n",
    "            d_t_d_inputs = self.weights\n",
    "            \n",
    "            #Gradients of loss against totals\n",
    "            d_l_d_t = gradient * d_out_d_t\n",
    "            \n",
    "            #Gradients of loss against weights/biases/input\n",
    "            d_l_d_w = d_t_d_w[np.newaxis].T @ d_l_d_t[np.newaxis]\n",
    "            d_l_d_b = d_l_d_t * d_t_d_b  \n",
    "            d_l_d_inputs = d_t_d_inputs @ d_l_d_t\n",
    "            \n",
    "            #update weights/biases\n",
    "            self.weights -= learn_rate * d_l_d_w\n",
    "            self.biases -= learn_rate * d_l_d_b\n",
    "            return d_l_d_inputs.reshape(self.last_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-09T16:11:02.975253Z",
     "iopub.status.busy": "2022-07-09T16:11:02.974421Z",
     "iopub.status.idle": "2022-07-09T16:11:23.033873Z",
     "shell.execute_reply": "2022-07-09T16:11:23.032269Z",
     "shell.execute_reply.started": "2022-07-09T16:11:02.975193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(784, 42000)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = pd.read_csv('digit-recognizer/train.csv')\n",
    "\n",
    "# X_train = data.iloc[:, 1:785].T / 255.\n",
    "# Y_train = data.T.iloc[1:785, 0]\n",
    "\n",
    "# X_train = np.array(X_train)\n",
    "# Y_train = np.array(Y_train)\n",
    "\n",
    "# np.random.shuffle(X_train) # shuffle before splitting into dev and training sets\n",
    "# np.random.shuffle(Y_train) # shuffle before splitting into dev and training sets\n",
    "\n",
    "# # data_dev = data[0:1000].T\n",
    "# # Y_dev = data_dev[0]\n",
    "# # X_dev = data_dev[1:n]\n",
    "# # X_dev = X_dev / 255.\n",
    "\n",
    "# # data_train = data[1000:m].T\n",
    "# # Y_train = data_train[0]\n",
    "# # X_train = data_train[1:n]\n",
    "# # X_train = X_train / 255.\n",
    "# print(Y_train.shape)\n",
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 ... 9 6 4]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(1000,)\n",
      "(784, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('digit-recognizer/train.csv')\n",
    "data = np.array(data)\n",
    "\n",
    "datatrain = data[0:1000].T\n",
    "Y = datatrain[0]\n",
    "Xtrain = datatrain[1:1000]\n",
    "Xtrain = Xtrain / 255.\n",
    "print(datatrain)\n",
    "print(Y.shape)\n",
    "Xtrain[0].shape\n",
    "\n",
    "datadev = data[1001:2001].T\n",
    "Ydev = datadev[0]\n",
    "Xdev = datadev[1:1000] / 255.\n",
    "print(Xdev.shape)\n",
    "Xdev[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-09T16:08:47.678020Z",
     "iopub.status.idle": "2022-07-09T16:08:47.678417Z",
     "shell.execute_reply": "2022-07-09T16:08:47.678254Z",
     "shell.execute_reply.started": "2022-07-09T16:08:47.678217Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST CNN initialized\n",
      "----EPOCH 1 ---\n",
      "[Step 100] Past 100 steps: Average Loss 2.321 | Accuracy: 12%\n",
      "[Step 200] Past 100 steps: Average Loss 2.360 | Accuracy: 4%\n",
      "[Step 300] Past 100 steps: Average Loss 2.337 | Accuracy: 8%\n",
      "[Step 400] Past 100 steps: Average Loss 2.344 | Accuracy: 12%\n",
      "[Step 500] Past 100 steps: Average Loss 2.299 | Accuracy: 18%\n",
      "[Step 600] Past 100 steps: Average Loss 2.321 | Accuracy: 12%\n",
      "[Step 700] Past 100 steps: Average Loss 2.346 | Accuracy: 9%\n",
      "----EPOCH 2 ---\n",
      "[Step 100] Past 100 steps: Average Loss 2.305 | Accuracy: 8%\n",
      "[Step 200] Past 100 steps: Average Loss 2.332 | Accuracy: 6%\n",
      "[Step 300] Past 100 steps: Average Loss 2.303 | Accuracy: 14%\n",
      "[Step 400] Past 100 steps: Average Loss 2.314 | Accuracy: 13%\n",
      "[Step 500] Past 100 steps: Average Loss 2.272 | Accuracy: 19%\n",
      "[Step 600] Past 100 steps: Average Loss 2.292 | Accuracy: 12%\n",
      "[Step 700] Past 100 steps: Average Loss 2.328 | Accuracy: 10%\n",
      "----EPOCH 3 ---\n",
      "[Step 100] Past 100 steps: Average Loss 2.300 | Accuracy: 7%\n",
      "[Step 200] Past 100 steps: Average Loss 2.318 | Accuracy: 11%\n",
      "[Step 300] Past 100 steps: Average Loss 2.279 | Accuracy: 14%\n",
      "[Step 400] Past 100 steps: Average Loss 2.297 | Accuracy: 14%\n",
      "[Step 500] Past 100 steps: Average Loss 2.251 | Accuracy: 16%\n",
      "[Step 600] Past 100 steps: Average Loss 2.271 | Accuracy: 10%\n",
      "[Step 700] Past 100 steps: Average Loss 2.313 | Accuracy: 8%\n"
     ]
    }
   ],
   "source": [
    "conv = Conv(8)\n",
    "pool = MaxPool()\n",
    "softmax = Softmax(13 * 13 * 8, 10)\n",
    "\n",
    "def forward(image, label):\n",
    "    # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier\n",
    "    # to work with. This is standard practice.\n",
    "    \n",
    "    out = conv.forward((image) - 0.5)\n",
    "    out = pool.forward(out)\n",
    "    out = softmax.forward(out)\n",
    "    \n",
    "    #calculate cross-entropy loss and accuracy\n",
    "    loss = -np.log(out[label])\n",
    "    acc = 1 if(np.argmax(out) == label) else 0\n",
    "    \n",
    "    return out, loss, acc\n",
    "\n",
    "def train(im, label, lr=0.005):\n",
    "    #forward\n",
    "    im.shape\n",
    "    out,loss,acc = forward(im, label)\n",
    "     \n",
    "    #calculate initial gradient\n",
    "    gradient = np.zeros(10)\n",
    "    gradient[label] = -1/out[label]\n",
    "    \n",
    "    \n",
    "    #Backprop\n",
    "    gradient = softmax.backprop(gradient, lr)\n",
    "    gradient = pool.backprop(gradient)\n",
    "    gradient = conv.backprop(gradient, lr)\n",
    "    \n",
    "    return np.argmax(out, 0), loss, acc\n",
    "    \n",
    "    \n",
    "print('MNIST CNN initialized')\n",
    "\n",
    "for epoch in range(3):\n",
    "    print('----EPOCH %d ---'%(epoch+1))\n",
    "    \n",
    "    #shuffle the training data\n",
    "    permutation = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[permutation]\n",
    "    Y_train = Y_train[permutation]\n",
    "\n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    for i, (im, label) in enumerate(zip(Xtrain, Y)):\n",
    "        #print stats every 100 steps\n",
    "        if(i>0 and i %100 == 99):\n",
    "            print('[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %(i + 1, loss / 100, num_correct))\n",
    "\n",
    "            loss = 0\n",
    "            num_correct = 0\n",
    "        im = im.reshape((28, 28))\n",
    "        out, l, acc = train(im, label)\n",
    "        loss += l\n",
    "        num_correct += acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the CNN\n",
      "Test Loss: 2.9377489138603443\n",
      "Test Accuracy: 0.11352040816326531\n"
     ]
    }
   ],
   "source": [
    "print('Testing the CNN')\n",
    "loss = 0\n",
    "num_correct = 0\n",
    "for im, label in zip(Xdev.T, Ydev.T):\n",
    "  im = im.reshape((28, 28))\n",
    "  _, l, acc = forward(im, label)\n",
    "  loss += l\n",
    "  num_correct += acc\n",
    "\n",
    "num_tests = len(Xdev)\n",
    "print('Test Loss:', loss / num_tests)\n",
    "print('Test Accuracy:', num_correct / num_tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
